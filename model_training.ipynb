{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sruthij93/Stock-Price-Prediction-Sentiment-Analysis/blob/main/model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Irf2PZFbba76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supabase\n",
        "!pip install pandas\n"
      ],
      "metadata": {
        "id": "a8YYOQPg-Jj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5PdQVdm9vO8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from supabase import create_client\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = userdata.get(\"SUPABASE_URL\")\n",
        "key = userdata.get(\"SUPABASE_KEY\")\n",
        "alphavantage_key = userdata.get(\"ALPHA_VANTAGE_API_KEY\")\n",
        "finnhub_key = userdata.get(\"FINNHUB_API_KEY\")\n",
        "supabase = create_client(url, key)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "GIp6SGWqoYiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Aut0FrUB-YZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch data from supabase from stock_prices table and for 'AAPL' ticker\n",
        "table = supabase.table(\"stock_prices\")\n",
        "response = table.select(\"*\").eq(\"ticker\", \"AAPL\").execute()\n"
      ],
      "metadata": {
        "id": "Ndm0tRO59bWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert response to dict\n",
        "data = response.data\n",
        "# type(data)\n",
        "# convert data to df\n",
        "df = pd.DataFrame(data)\n",
        "df.tail(10)"
      ],
      "metadata": {
        "id": "MFKkuvy29pFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "MdcnMrj1xPZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns = ['sentiment_score'], inplace=True)"
      ],
      "metadata": {
        "id": "Mvcp9iuJ-OnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GV_1a3Lg9vPC"
      },
      "outputs": [],
      "source": [
        "# create a day and month column\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['day'] = df['date'].dt.day\n",
        "df['month'] = df['date'].dt.month\n",
        "# add a flag to check if there is a sentiment score or not based on num_articles\n",
        "df['has_sentiment'] = df['num_articles'].apply(lambda x: 1 if x>0 else 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail(5)"
      ],
      "metadata": {
        "id": "OTdccfjD_ZPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "xpI9tXohz8h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "5T18xiGH0EcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Two Models:\n",
        "1. Using only price data\n",
        "2. Using price + sentiment features"
      ],
      "metadata": {
        "id": "2CHW9wP-_kT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# features for both models\n",
        "price_features = ['open', 'high', 'low', 'close', 'volume', 'day', 'month']\n",
        "sentiment_features = ['avg_sentiment', 'sentiment_ma3', 'sentiment_lag1', 'log_article_count', 'sentiment_volatility', 'sentiment_close_corr', 'has_sentiment']"
      ],
      "metadata": {
        "id": "6PJWySUh_gi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[price_features].values.shape"
      ],
      "metadata": {
        "id": "B2P6bKSX_qUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create datasets\n",
        "X_price = df[price_features].values\n",
        "X_sentiment = df[price_features + sentiment_features].values"
      ],
      "metadata": {
        "id": "Nuprn4r1_sDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_price.shape)\n",
        "print(X_sentiment.shape)"
      ],
      "metadata": {
        "id": "DUSGMewcST1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating targets for next 5 days\n",
        "y_multi = np.column_stack([\n",
        "    df['close'].shift(-1).values,\n",
        "    df['close'].shift(-2).values,\n",
        "    df['close'].shift(-3).values,\n",
        "    df['close'].shift(-4).values,\n",
        "    df['close'].shift(-5).values\n",
        "])\n",
        "# fill NANs in the last 5 rows with last known close price\n",
        "fill_value = df['close'].iloc[-1]\n",
        "for i in range(5):\n",
        "    y_multi[-5+i][-(i+1):] = fill_value"
      ],
      "metadata": {
        "id": "NWzb2g8z_uMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_multi[-5:]"
      ],
      "metadata": {
        "id": "Eezw1jdQ__sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fn to create sequences for LSTM\n",
        "def create_sequences(data, targets, lookback=30):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(data) - lookback):\n",
        "        X_seq.append(data[i:i+lookback])\n",
        "        y_seq.append(targets[i+lookback])\n",
        "    return np.array(X_seq), np.array(y_seq)"
      ],
      "metadata": {
        "id": "x9Bfbg2EAB_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale the data\n",
        "scaler_price = MinMaxScaler()\n",
        "scaler_sentiment = MinMaxScaler()\n",
        "scaler_sentiment_only = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n"
      ],
      "metadata": {
        "id": "rjT0yTqjAFEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the scalers\n",
        "X_price_scaled = scaler_price.fit_transform(X_price)\n",
        "X_with_sentiment_scaled = scaler_sentiment.fit_transform(X_sentiment)\n",
        "y_scaled = scaler_y.fit_transform(y_multi)\n",
        "y_single_scaled = scaler_y_single.fit_transform(y_single.reshape(-1, 1))\n"
      ],
      "metadata": {
        "id": "p4sxyhxoAR5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle dump the scalers\n",
        "import pickle\n",
        "with open('scaler_price.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_price, f)\n",
        "with open('scaler_sentiment.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_sentiment, f)\n",
        "with open('scaler_y.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_y, f)"
      ],
      "metadata": {
        "id": "rjNBfOrmfIss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create sequences of 30 days\n",
        "X_price_seq, y_price_seq = create_sequences(X_price_scaled, y_scaled, lookback=30)\n",
        "X_sentiment_seq, y_sentiment_seq = create_sequences(X_with_sentiment_scaled, y_scaled, lookback=30)\n"
      ],
      "metadata": {
        "id": "y33CUvOcAOTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_price_seq.shape"
      ],
      "metadata": {
        "id": "C6EuQ5KCjaoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split data without shuffling (80% train, 10% validation, 10% test)\n",
        "total_size = len(X_price_seq)\n",
        "train_end = int(total_size * 0.8)\n",
        "val_end = int(total_size * 0.9)\n",
        "\n",
        "X_price_train = X_price_seq[:train_end]\n",
        "X_price_val = X_price_seq[train_end:val_end]\n",
        "X_price_test = X_price_seq[val_end:]\n",
        "y_price_train = y_price_seq[:train_end]\n",
        "y_price_val = y_price_seq[train_end:val_end]\n",
        "y_price_test = y_price_seq[val_end:]\n",
        "\n",
        "X_sentiment_train = X_sentiment_seq[:train_end]\n",
        "X_sentiment_val = X_sentiment_seq[train_end:val_end]\n",
        "X_sentiment_test = X_sentiment_seq[val_end:]\n",
        "y_sentiment_train = y_sentiment_seq[:train_end]\n",
        "y_sentiment_val = y_sentiment_seq[train_end:val_end]\n",
        "y_sentiment_test = y_sentiment_seq[val_end:]\n",
        "\n"
      ],
      "metadata": {
        "id": "d5Nu-xNqAxiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_price_train.shape"
      ],
      "metadata": {
        "id": "wQp040XezNSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_price_test.shape"
      ],
      "metadata": {
        "id": "1_iV9M4vx1dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 1: Price only model\n",
        "model_price = Sequential([\n",
        "    LSTM(128, return_sequences = True, input_shape = (30, X_price_train.shape[2])),\n",
        "    Dropout(0.4),\n",
        "    LSTM(64),\n",
        "    Dropout(0.4),\n",
        "    Dense(16, activation = 'relu'),\n",
        "    Dense(5)\n",
        "])\n",
        "model_price.compile(optimizer='adam', loss='mse', metrics=['mae'])"
      ],
      "metadata": {
        "id": "H0Ly1ciZA0dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2: With sentiment\n",
        "model_with_sentiment = Sequential([\n",
        "    LSTM(32, return_sequences = True, input_shape = (30, X_sentiment_train.shape[2])),\n",
        "    Dropout(0.4),\n",
        "    LSTM(32),\n",
        "    Dropout(0.4),\n",
        "    Dense(16, activation = 'relu'),\n",
        "    Dense(5)\n",
        "])\n",
        "model_with_sentiment.compile(optimizer='adam', loss='mse', metrics=['mae'])"
      ],
      "metadata": {
        "id": "1zxEHUn7A4_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# early stopping to prevent overfitting and model uses best weights\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "uDfjKm2VA_o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model 1 (price only)\n",
        "print(\"Training price-only model: \")\n",
        "history_price = model_price.fit(\n",
        "    X_price_train, y_price_train,\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "ZraOrLAhBBz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model 2 (with sentiment)\n",
        "print(\"Training model with sentiment: \")\n",
        "history_sentiment = model_with_sentiment.fit(\n",
        "    X_sentiment_train, y_sentiment_train,\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    validation_split = 0.2,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "fyzsmo3rtNxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate models\n",
        "price_mae = model_price.evaluate(X_price_test, y_price_test)[1]\n",
        "sentiment_mae = model_with_sentiment.evaluate(X_sentiment_test, y_sentiment_test)[1]\n",
        "\n",
        "print(f\"Price-only model MAE: {price_mae:.4f}\")\n",
        "print(f\"Sentiment model MAE: {sentiment_mae:.4f}\")"
      ],
      "metadata": {
        "id": "DZRkNE8eBZ07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best validation MAE for Price model: {min(history_price.history['val_mae']):.4f}\")\n",
        "print(f\"Best validation MAE for Sentiment model: {min(history_sentiment.history['val_mae']):.4f}\")\n"
      ],
      "metadata": {
        "id": "PpzX2fcQpNWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to make predictions for the next 5 days\n",
        "def predict_next_5_days(model, recent, scaler_x, scaler_y):\n",
        "\n",
        "    scaled_input = scaler_x.transform(recent)\n",
        "\n",
        "    # reshape to get one prediction: 30 days & feature shape\n",
        "    reshaped_input = scaled_input.reshape(1, 30, scaled_input.shape[1])\n",
        "    print(reshaped_input.shape)\n",
        "\n",
        "    scaled_prediction = model.predict(reshaped_input)\n",
        "\n",
        "    # inverse transform to get actual prices\n",
        "    prediction = scaler_y.inverse_transform(scaled_prediction)\n",
        "\n",
        "    return prediction[0]"
      ],
      "metadata": {
        "id": "rHjP_VKKBgEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# last 30 days of data for prediction\n",
        "latest_window_price = X_price[-30:]\n",
        "latest_window_sentiment = X_sentiment[-30:]\n"
      ],
      "metadata": {
        "id": "RkrXHRKPBnk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latest_window_sentiment.shape"
      ],
      "metadata": {
        "id": "gKiV_PeNTBtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict with both models\n",
        "price_prediction = predict_next_5_days(model_price, latest_window_price, scaler_price, scaler_y)\n",
        "sentiment_prediction = predict_next_5_days(model_with_sentiment, latest_window_sentiment, scaler_sentiment, scaler_y)\n"
      ],
      "metadata": {
        "id": "Y5vpX0-0Brtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"5 day Prediction:\")\n",
        "print(\"Day |   Price Model    | Sentiment Model\")\n",
        "for i in range(5):\n",
        "    print(f\" {i+1}  |     ${price_prediction[i]:.2f}      |     ${sentiment_prediction[i]:.2f} \")"
      ],
      "metadata": {
        "id": "IJvKeacRBvl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle dump both models\n",
        "import pickle\n",
        "with open('model_price.pkl', 'wb') as f:\n",
        "    pickle.dump(model_price, f)\n",
        "with open('model_with_sentiment.pkl', 'wb') as f:\n",
        "    pickle.dump(model_with_sentiment, f)"
      ],
      "metadata": {
        "id": "p6dRCGUeG0UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot training history\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_price.history['loss'], label='Price Model Training Loss')\n",
        "plt.plot(history_price.history['val_loss'], label='Price Model Validation Loss')\n",
        "plt.plot(history_sentiment.history['loss'], label='Sentiment Model Training Loss')\n",
        "plt.plot(history_sentiment.history['val_loss'], label='Sentiment Model Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_price.history['mae'], label='Price Model Training MAE')\n",
        "plt.plot(history_price.history['val_mae'], label='Price Model Validation MAE')\n",
        "plt.plot(history_sentiment.history['mae'], label='Sentiment Model Training MAE')\n",
        "plt.plot(history_sentiment.history['val_mae'], label='Sentiment Model Validation MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.ylabel('MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "YcDhZaF8B73x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions vs actual values for the test set\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title('Stock Price Predictions on Test Set (next 5 days)')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Price ($)')\n",
        "\n",
        "# actual test set values\n",
        "y_test_actual = scaler_y.inverse_transform(y_price_test)\n",
        "\n",
        "# predictions on test set for price model\n",
        "y_test_pred = model_price.predict(X_price_test)\n",
        "y_test_pred = scaler_y.inverse_transform(y_test_pred)\n",
        "\n",
        "# prediction for the sentiment model\n",
        "y_test_pred_sentiment = model_with_sentiment.predict(X_sentiment_test)\n",
        "y_test_pred_sentiment = scaler_y.inverse_transform(y_test_pred_sentiment)\n",
        "\n",
        "\n",
        "# actual vs predicted plot (just 1st prediction from the 5 day prediction window)\n",
        "plt.plot(y_test_actual[:, 0], label='Actual Prices', color='blue')\n",
        "plt.plot(y_test_pred[:, 0], label='Price Model Predictions', color='red', linestyle='--')\n",
        "plt.plot(y_test_pred_sentiment[:, 0], label='Sentiment Model Predictions', color='green', linestyle=':')\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a scatter plot of actual vs predicted values (just 1st prediction from the 5 day prediction window)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test_actual[:, 0], y_test_pred[:, 0], alpha=0.5, label='Price Model')\n",
        "plt.scatter(y_test_actual[:, 0], y_test_pred_sentiment[:, 0], alpha=0.5, label='Sentiment Model')\n",
        "\n",
        "plt.plot([y_test_actual.min(), y_test_actual.max()], [y_test_actual.min(), y_test_actual.max()], 'k--', lw=2)\n",
        "plt.xlabel('Actual Prices')\n",
        "plt.ylabel('Predicted Prices')\n",
        "plt.title('Actual vs Predicted Prices on Test Set')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GgrY3OczC3sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions for each of the 5 days\n",
        "days = ['Day 1', 'Day 2', 'Day 3', 'Day 4', 'Day 5']\n",
        "fig, axes = plt.subplots(5, 1, figsize=(12, 15), sharex=True)\n",
        "\n",
        "for i in range(5):\n",
        "    axes[i].plot(y_test_actual[:, i], label='Actual', color='blue')\n",
        "    axes[i].plot(y_test_pred[:, i], label='Price Model', color='red', linestyle='--')\n",
        "    axes[i].plot(y_test_pred_sentiment[:, i], label='Sentiment Model', color='green', linestyle=':')\n",
        "    axes[i].set_title(f'Predictions for {days[i]}')\n",
        "    axes[i].set_ylabel('Price ($)')\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True)\n",
        "\n",
        "plt.xlabel('Time')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yL91YauNDFAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single Day Prediction:\n",
        "Models trained (price only and sentiment + price) for predicting the stock price for a single day."
      ],
      "metadata": {
        "id": "h_FUwi_4JLDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "# Create single-day target\n",
        "y_single = df['close'].values\n",
        "\n",
        "\n",
        "# fn to create sequences for LSTM\n",
        "def create_sequences_single(data, targets, lookback=30):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(data) - lookback):\n",
        "        X_seq.append(data[i:i+lookback])\n",
        "        y_seq.append(targets[i+lookback])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# scale the data\n",
        "scaler_price_single = MinMaxScaler()\n",
        "scaler_sentiment_single = MinMaxScaler()\n",
        "scaler_y_single = MinMaxScaler()\n",
        "\n",
        "# fit the scalers\n",
        "X_price_scaled_single = scaler_price_single.fit_transform(X_price)\n",
        "X_sentiment_scaled_single = scaler_sentiment_single.fit_transform(X_sentiment)\n",
        "y_scaled_single = scaler_y_single.fit_transform(y_single.reshape(-1, 1))\n",
        "\n",
        "# create sequences\n",
        "X_price_seqs, y_price_seqs = create_sequences_single(X_price_scaled_single, y_scaled_single, lookback=30)\n",
        "X_sentiment_seqs, y_sentiment_seqs = create_sequences_single(X_sentiment_scaled_single, y_scaled_single, lookback=30)\n",
        "\n",
        "# split the data without shuffling (80% train, 20% test)\n",
        "train_size_prices = int(len(X_price_seqs) * 0.8)\n",
        "X_price_trains = X_price_seqs[:train_size_prices]\n",
        "X_price_tests = X_price_seqs[train_size_prices:]\n",
        "y_price_trains = y_price_seqs[:train_size_prices]\n",
        "y_price_tests = y_price_seqs[train_size_prices:]\n",
        "\n",
        "train_size_sentiments = int(len(X_sentiment_seqs) * 0.8)\n",
        "X_sentiment_trains = X_sentiment_seqs[:train_size_sentiments]\n",
        "X_sentiment_tests = X_sentiment_seqs[train_size_sentiments:]\n",
        "y_sentiment_trains = y_sentiment_seqs[:train_size_sentiments]\n",
        "y_sentiment_tests = y_sentiment_seqs[train_size_sentiments:]\n",
        "\n",
        "\n",
        "# Model 1: Price only model\n",
        "model_price_1 = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(30, X_price_trains.shape[2])),\n",
        "    Dropout(0.3),\n",
        "    LSTM(32),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model_price_1.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Model 2: Including sentiment\n",
        "model_with_sentiment_1 = Sequential([\n",
        "    LSTM(32, return_sequences=True, input_shape=(30, X_sentiment_trains.shape[2])),\n",
        "    Dropout(0.3),\n",
        "    LSTM(32),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model_with_sentiment_1.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train Model 1 (price only)\n",
        "print(\"Training price-only model...\")\n",
        "history_price_1 = model_price_1.fit(\n",
        "    X_price_trains, y_price_trains,\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train Model 2 (with sentiment)\n",
        "print(\"Training model with sentiment...\")\n",
        "history_sentiment_1 = model_with_sentiment_1.fit(\n",
        "    X_sentiment_trains, y_sentiment_trains,\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# model evaluation\n",
        "price_mae_1 = model_price_1.evaluate(X_price_tests, y_price_tests)[1]\n",
        "sentiment_mae_1 = model_with_sentiment_1.evaluate(X_sentiment_tests, y_sentiment_tests)[1]\n",
        "\n",
        "print(f\"Price-only model MAE: {price_mae_1:.4f}\")\n",
        "print(f\"Sentiment model MAE: {sentiment_mae_1:.4f}\")\n",
        "\n",
        "\n",
        "# function to make predctions for the next day\n",
        "def predict_next_day(model, recent_data, scaler_x, scaler_y):\n",
        "\n",
        "    scaled_input = scaler_x.transform(recent_data)\n",
        "    print(scaled_input.shape)\n",
        "\n",
        "    # reshape to get one prediction\n",
        "    reshaped_input = scaled_input.reshape(1, 30, scaled_input.shape[1])\n",
        "\n",
        "    scaled_prediction = model.predict(reshaped_input)\n",
        "\n",
        "    # inverse transform to get actual price\n",
        "    prediction = scaler_y.inverse_transform(scaled_prediction)\n",
        "\n",
        "    return prediction[0][0]\n",
        "\n",
        "# last 30 days of data for prediction\n",
        "latest_window_price = X_price[-30:]\n",
        "latest_window_sentiment = X_sentiment[-30:]\n",
        "\n",
        "# predictions with both models\n",
        "price_prediction_1 = predict_next_day(model_price_1, latest_window_price, scaler_price_single, scaler_y_single)\n",
        "sentiment_prediction_1 = predict_next_day(model_with_sentiment_1, latest_window_sentiment, scaler_sentiment_single, scaler_y_single)\n",
        "\n",
        "print(\"\\nPredictions for the next day:\")\n",
        "print(f\"Price-only model: ${price_prediction_1:.2f}\")\n",
        "print(f\"Sentiment model: ${sentiment_prediction_1:.2f}\")"
      ],
      "metadata": {
        "id": "PTbwImNAJIlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the predictions vs actual values for test set\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title('Stock Price (single day) Predictions on Test Set')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Price ($)')\n",
        "\n",
        "y_test_actuals = scaler_y_single.inverse_transform(y_price_tests)\n",
        "\n",
        "# price only model\n",
        "y_test_preds = model_price_1.predict(X_price_tests)\n",
        "y_test_preds = scaler_y_single.inverse_transform(y_test_preds)\n",
        "\n",
        "# sentiment model\n",
        "y_test_pred_sentiments = model_with_sentiment_1.predict(X_sentiment_tests)\n",
        "y_test_pred_sentiments = scaler_y_single.inverse_transform(y_test_pred_sentiments)\n",
        "\n",
        "# Plot actual vs predicted\n",
        "plt.plot(y_test_actuals, label='Actual Prices', color='blue')\n",
        "plt.plot(y_test_preds, label='Price Model Predictions', color='red', linestyle='--')\n",
        "plt.plot(y_test_pred_sentiments, label='Sentiment Model Predictions', color='green', linestyle=':')\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b6DweamN_jW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scatter plot of actual vs predicted values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test_actuals[:, 0], y_test_preds[:, 0], alpha=0.5, label='Price Model')\n",
        "plt.scatter(y_test_actuals[:, 0], y_test_pred_sentiments[:, 0], alpha=0.5, label='Sentiment Model')\n",
        "\n",
        "plt.plot([y_test_actuals.min(), y_test_actuals.max()], [y_test_actuals.min(), y_test_actuals.max()], 'k--', lw=2)\n",
        "plt.xlabel('Actual Prices')\n",
        "plt.ylabel('Predicted Prices')\n",
        "plt.title('Actual vs Predicted Prices on Test Set (single day)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VUfOcFIG-MRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "NyW7VI1w2oyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model_with_sentiment_1.pkl', 'wb') as file:\n",
        "  pickle.dump(model_with_sentiment_1, file)"
      ],
      "metadata": {
        "id": "HDvAiL09jH6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model_price_1.pkl', 'wb') as file:\n",
        "  pickle.dump(model_price_1, file)"
      ],
      "metadata": {
        "id": "Ub6VOFDejUgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle dump scalers for single day prediction\n",
        "with open('scaler_price_single.pkl', 'wb') as file:\n",
        "  pickle.dump(scaler_price_single, file)\n",
        "\n",
        "with open('scaler_sentiment_single.pkl', 'wb') as file:\n",
        "  pickle.dump(scaler_sentiment_single, file)\n",
        "\n",
        "with open('scaler_y_single.pkl', 'wb') as file:\n",
        "  pickle.dump(scaler_y_single, file)\n"
      ],
      "metadata": {
        "id": "bqUkXZfB8Dnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_______________________________________________________________________________________"
      ],
      "metadata": {
        "id": "dJ7MvVYsKF6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Testing (in progress)"
      ],
      "metadata": {
        "id": "nmLEInbqIoJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install finnhub-python"
      ],
      "metadata": {
        "id": "LBEAY1Exrtid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import finnhub\n",
        "from supabase import create_client\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "RB5UG6hui-Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request, json\n",
        "import datetime as dt"
      ],
      "metadata": {
        "id": "o5uR_xD8TyUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN FUNCTION\n",
        "# %%writefile main.py\n",
        "# saves python file in colab as main.py\n",
        "\n",
        "from load_dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import urllib.request, json\n",
        "import datetime as dt\n",
        "import finnhub\n",
        "from supabase import create_client\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "url= os.environ.get(\"SUPABASE_URL\")\n",
        "key= os.environ.get(\"SUPABASE_KEY\")\n",
        "supabase= create_client(url, key)\n",
        "\n",
        "API_KEY = os.getenv(\"ALPHA_VANTAGE_API_KEY\")\n",
        "finnhub_api_key = os.getenv(\"FINNHUB_API_KEY\")\n",
        "finnhub_client = finnhub.Client(api_key=finnhub_api_key)\n",
        "\n",
        "# Model and tokenizer (FinBERT) for sentiment analysis\n",
        "model_name = \"ProsusAI/finbert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def get_test_stock_data(symbol):\n",
        "  url_string = \"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=%s&outputsize=full&apikey=%s\"%(symbol, alphavantage_key)\n",
        "  with urllib.request.urlopen(url_string) as url:\n",
        "        # read the data from the url\n",
        "        data = json.loads(url.read().decode())\n",
        "\n",
        "        # key = \"Time Series (Daily)\" only needs to be used\n",
        "        data = data['Time Series (Daily)']\n",
        "\n",
        "        # create a pandas dataframe with the data\n",
        "        df = pd.DataFrame.from_dict(data, orient = 'index')\n",
        "        df.reset_index(inplace = True)\n",
        "        df.rename(columns={'index': 'date', '1. open': 'open', '2. high': 'high', '3. low': 'low', '4. close': 'close', '5. volume': 'volume'}, inplace=True)\n",
        "\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "        # get past 30 DAYS data\n",
        "        thirty_days = dt.datetime.today() - dt.timedelta(days=30)\n",
        "        df = df[df['date'] > thirty_days]\n",
        "\n",
        "        df['open'] = df['open'].astype(float)\n",
        "        df['high'] = df['high'].astype(float)\n",
        "        df['low'] = df['low'].astype(float)\n",
        "        df['close'] = df['close'].astype(float)\n",
        "        df['volume'] = df['volume'].astype(int)\n",
        "\n",
        "        # Sort the dataframe by date\n",
        "        df.sort_values(by='date', ascending=True, inplace=True)\n",
        "\n",
        "        df['ticker'] = symbol.upper()\n",
        "\n",
        "\n",
        "        # print(df.head(5))\n",
        "\n",
        "        return df\n",
        "\n",
        "def fetch_news_test(symbol):\n",
        "  to_str = (dt.datetime.now() - dt.timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "  from_str = (dt.datetime.today() - dt.timedelta(days=31)).strftime('%Y-%m-%d')\n",
        "\n",
        "  url = f'https://finnhub.io/api/v1/company-news?symbol={symbol}&from={from_str}&to={to_str}&token={finnhub_key}'\n",
        "  with urllib.request.urlopen(url) as url:\n",
        "    data = json.loads(url.read().decode())\n",
        "\n",
        "  news_df = pd.DataFrame(data)\n",
        "  news_df['ticker'] = symbol.upper()\n",
        "  news_df['date'] = pd.to_datetime(news_df['datetime'], unit='s').dt.strftime('%Y-%m-%d')\n",
        "  news_df = news_df[['ticker', 'date', 'headline', 'summary', 'url']]\n",
        "  news_df.sort_values(by='date', ascending=True, inplace=True)\n",
        "  news_df.reset_index(drop=True, inplace=True)\n",
        "  return news_df\n",
        "\n",
        "\n",
        "def sentiment_analysis_test(news_df):\n",
        "  texts = (news_df['headline'].fillna('') + '. ' + news_df['summary'].fillna('')).tolist()\n",
        "  sentiments = sentiment_pipeline(texts, padding = True, truncation=True, batch_size=16)\n",
        "  scores = [\n",
        "      r['score'] if r['label'] == 'positive' else -r['score']\n",
        "      for r in sentiments\n",
        "  ]\n",
        "  return scores\n"
      ],
      "metadata": {
        "id": "64e5dECoS5_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_stock_data(symbol):\n",
        "  url_string = \"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=%s&outputsize=full&apikey=%s\"%(symbol, alphavantage_key)\n",
        "  with urllib.request.urlopen(url_string) as url:\n",
        "        # read the data from the url\n",
        "        data = json.loads(url.read().decode())\n",
        "\n",
        "        # key = \"Time Series (Daily)\" only needs to be used\n",
        "        data = data['Time Series (Daily)']\n",
        "\n",
        "        # create a pandas dataframe with the data\n",
        "        df = pd.DataFrame.from_dict(data, orient = 'index')\n",
        "        df.reset_index(inplace = True)\n",
        "        df.rename(columns={'index': 'date', '1. open': 'open', '2. high': 'high', '3. low': 'low', '4. close': 'close', '5. volume': 'volume'}, inplace=True)\n",
        "\n",
        "\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "        # get only past 30 DAYS data\n",
        "        thirty_days = dt.datetime.today() - dt.timedelta(days=30)\n",
        "        df = df[df['date'] > thirty_days]\n",
        "\n",
        "        # convert rest of the columns to the appropriate data types\n",
        "        df['open'] = df['open'].astype(float)\n",
        "        df['high'] = df['high'].astype(float)\n",
        "        df['low'] = df['low'].astype(float)\n",
        "        df['close'] = df['close'].astype(float)\n",
        "        df['volume'] = df['volume'].astype(int)\n",
        "\n",
        "        # Sort the dataframe by date\n",
        "        df.sort_values(by='date', ascending=True, inplace=True)\n",
        "\n",
        "        df['ticker'] = symbol.upper()\n",
        "\n",
        "\n",
        "        # print(df.head(5))\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "GHdq8gtESiiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df = get_test_stock_data('AAPL')"
      ],
      "metadata": {
        "id": "SagC7QW4UI2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.tail(10)"
      ],
      "metadata": {
        "id": "sqIliMd8UNWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.shape"
      ],
      "metadata": {
        "id": "gUR3nveYZDzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import datetime\n",
        "# import pytz\n",
        "\n",
        "# os.environ['TZ'] = 'America/Los_Angeles'\n",
        "# dt.datetime.now(pytz.timezone(os.environ['TZ'])).strftime('%Y-%m-%d')"
      ],
      "metadata": {
        "id": "_kjE5ArXWU33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_news_test(symbol):\n",
        "  to_str = (dt.datetime.now() - dt.timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "  from_str = (dt.datetime.today() - dt.timedelta(days=31)).strftime('%Y-%m-%d')\n",
        "\n",
        "  url = f'https://finnhub.io/api/v1/company-news?symbol={symbol}&from={from_str}&to={to_str}&token={finnhub_key}'\n",
        "  with urllib.request.urlopen(url) as url:\n",
        "    data = json.loads(url.read().decode())\n",
        "\n",
        "  news_df = pd.DataFrame(data)\n",
        "  news_df['ticker'] = symbol.upper()\n",
        "  news_df['date'] = pd.to_datetime(news_df['datetime'], unit='s').dt.strftime('%Y-%m-%d')\n",
        "  news_df = news_df[['ticker', 'date', 'headline', 'summary', 'url']]\n",
        "  news_df.sort_values(by='date', ascending=True, inplace=True)\n",
        "  news_df.reset_index(drop=True, inplace=True)\n",
        "  return news_df\n",
        "\n"
      ],
      "metadata": {
        "id": "SxmekpYmUTXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio patch for Jupyter notebooks\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "fVUcluJ1CTeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to fetch news for a date chunk asynchronously\n",
        "async def fetch_news_chunk_test(session, symbol, from_str, to_str):\n",
        "\n",
        "    url = f'https://finnhub.io/api/v1/company-news?symbol={symbol}&from={from_str}&to={to_str}&token={finnhub_key}'\n",
        "    try:\n",
        "        async with session.get(url, timeout=10) as response:\n",
        "            if response.status == 200:\n",
        "                return await response.json()\n",
        "            print(f\"Error {response.status} for {from_str} to {to_str} for {symbol}\")\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(f\"Request failed: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# function that calls fetch_news_chunk asynchronously\n",
        "async def fetch_30_days_test(session, symbol):\n",
        "\n",
        "    end_date = dt.datetime.today()\n",
        "    start_date = end_date - dt.timedelta(days=30)\n",
        "    all_articles = []\n",
        "\n",
        "\n",
        "    # Generate date chunks\n",
        "    current_start = start_date\n",
        "    while current_start <= end_date:\n",
        "        current_end = min(current_start + dt.timedelta(days=5), end_date)\n",
        "        from_str = current_start.strftime('%Y-%m-%d')\n",
        "        to_str = current_end.strftime('%Y-%m-%d')\n",
        "        # date_chunks.append((from_str, to_str))\n",
        "\n",
        "        chunk = await fetch_news_chunk_test(session, symbol, from_str, to_str)\n",
        "\n",
        "        if isinstance(chunk, list):\n",
        "            all_articles.extend(chunk)\n",
        "            print(f\"{symbol}: Got {len(chunk)} articles from {from_str} to {to_str}\")\n",
        "            if len(chunk) >= 1000:\n",
        "                print(f\"Warning: Possible data truncation in chunk\")\n",
        "\n",
        "        current_start = current_end + dt.timedelta(days=1)\n",
        "\n",
        "    df = pd.DataFrame(all_articles).drop_duplicates('id')\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'], unit='s', errors='coerce')\n",
        "    df = df.dropna(subset=['datetime'])\n",
        "    df = df[df['datetime'].between('1970-01-01', '2262-04-11')]\n",
        "    df['ticker'] = symbol.upper()\n",
        "    df['date'] = pd.to_datetime(df['datetime']).dt.date\n",
        "    # df = df.groupby(['ticker', 'date']).head(10).reset_index(drop=True)\n",
        "    # print(df.head(3))\n",
        "    # print(df.iloc[0]['headline'])\n",
        "    df = df[['ticker', 'id', 'datetime', 'headline', 'url', 'summary']].sort_values('datetime')\n",
        "    return df"
      ],
      "metadata": {
        "id": "pjMlqHfvvtDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main(symbol):\n",
        "\n",
        "  async with aiohttp.ClientSession() as session:\n",
        "    news_df = await fetch_30_days_test(session, symbol)\n",
        "    return news_df\n",
        "\n",
        "\n",
        "symbol = 'AAPL'\n",
        "news = asyncio.run(main(symbol))\n",
        "print(news.head())"
      ],
      "metadata": {
        "id": "WaoVndAoNtm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "april_news = fetch_news_test('AAPL')"
      ],
      "metadata": {
        "id": "BHwu-v1qoLZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "april_news.tail(5)"
      ],
      "metadata": {
        "id": "_ZFCVHNVoOlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = fetch_news_test('AAPL')"
      ],
      "metadata": {
        "id": "ImbtECUkYEfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "news.head(5)"
      ],
      "metadata": {
        "id": "M-bHT7ZvYFr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and tokenizer (FinBERT) for sentiment analysis\n",
        "model_name = \"ProsusAI/finbert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "OrWscbrVi4eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_analysis_test(news_df):\n",
        "  texts = (news_df['headline'].fillna('') + '. ' + news_df['summary'].fillna('')).tolist()\n",
        "  sentiments = sentiment_pipeline(texts, padding = True, truncation=True, batch_size=16)\n",
        "  scores = [\n",
        "      r['score'] if r['label'] == 'positive' else -r['score']\n",
        "      for r in sentiments\n",
        "  ]\n",
        "  return scores\n"
      ],
      "metadata": {
        "id": "0OQoXeUodLrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "scores = sentiment_analysis_test(news)\n"
      ],
      "metadata": {
        "id": "UIxy2zE1j7BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(scores)"
      ],
      "metadata": {
        "id": "yrgg06HTk7Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news['sentiment_score'] = scores\n",
        "news['date'] = pd.to_datetime(news['datetime']).dt.date\n",
        "daily_sentiment_test = news.groupby(['date', 'ticker'])['sentiment_score'].mean().reset_index()\n",
        "daily_sentiment_test['num_articles'] = news.groupby(['ticker', 'date'])['date'].count().values\n",
        "stock_df['date'] = pd.to_datetime(stock_df['date']).dt.date\n",
        "\n",
        "# merge stock prices with daily average sentiment\n",
        "test_df = pd.merge(stock_df, daily_sentiment_test, on=['ticker', 'date'], how='left')\n",
        "test_df.rename(columns= {\"sentiment_score\" :\"avg_sentiment\"}, inplace=True)\n",
        "\n",
        "test_df['avg_sentiment']= test_df['avg_sentiment'].fillna(0.0)  # Fill missing sentiment as neutral\n",
        "test_df['num_articles'] = test_df['num_articles'].fillna(0)\n",
        "\n",
        "\n",
        "\n",
        "# Adding more features related to sentiment (lagged sentiment, moving averages, etc.)\n",
        "test_df['sentiment_ma3'] = test_df['avg_sentiment'].rolling(window=3).mean()\n",
        "test_df['sentiment_lag1'] = test_df['avg_sentiment'].shift(1)\n",
        "test_df['log_article_count'] = np.log(test_df['num_articles'] + 1)\n",
        "test_df['sentiment_volatility'] = test_df['avg_sentiment'].rolling(window=7).std()\n",
        "test_df['sentiment_close_corr'] = test_df['avg_sentiment'].rolling(5).corr(test_df['close'])\n",
        "\n",
        "test_df.head(5)"
      ],
      "metadata": {
        "id": "JUzx-zndlst4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.info()"
      ],
      "metadata": {
        "id": "DAKpKQqFcitn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_sentiment_test.shape"
      ],
      "metadata": {
        "id": "xCX8L4L6ZKNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a day and month column\n",
        "test_df['date'] = pd.to_datetime(test_df['date'])\n",
        "test_df['day'] = test_df['date'].dt.day\n",
        "test_df['month'] = test_df['date'].dt.month\n",
        "# Adding a flag to check if there is a sentiment score or not\n",
        "# based on the number of news articles, set the flag\n",
        "test_df['has_sentiment'] = test_df['num_articles'].apply(lambda x: 1 if x>0 else 0)"
      ],
      "metadata": {
        "id": "Cz-wl1akcKvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_price = test_df.drop(columns=['date','avg_sentiment', 'sentiment_ma3', 'sentiment_lag1', 'log_article_count', 'sentiment_volatility', 'sentiment_close_corr', 'has_sentiment'])\n",
        "test_df_sentiment = test_df.drop(columns=['date','open', 'high', 'low', 'close', 'volume', 'day', 'month'])"
      ],
      "metadata": {
        "id": "0Ksm2hU2YbrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model from pkl file\n",
        "with open('/content/drive/MyDrive/model_price_1.pkl', 'rb') as file:\n",
        "  model_price = pickle.load(file)\n",
        "\n",
        "with open('/content/drive/MyDrive/model_with_sentiment_1.pkl', 'rb') as file:\n",
        "  model_sentiment = pickle.load(file)\n",
        "\n",
        "test_df_price = test_df_price.values\n",
        "test_df_sentiment = test_df_sentiment.values\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eeoZ4D8rnnFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to make predictions for the next 5 days\n",
        "def predict_next_5_days_test(model, recent_data, scaler_x, scaler_y):\n",
        "\n",
        "    scaled_input = scaler_x.transform(recent_data)\n",
        "\n",
        "    reshaped_input = scaled_input.reshape(1, 21, scaled_input.shape[1])\n",
        "\n",
        "    # Make prediction\n",
        "    scaled_prediction = model.predict(reshaped_input)\n",
        "\n",
        "    # Inverse transform to get actual prices\n",
        "    prediction = scaler_y.inverse_transform(scaled_prediction)\n",
        "\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "rZO60ZdAhB7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict prices for next days\n",
        "price_prediction = predict_next_5_days_test(model_price, test_df_price, scaler_price, scaler_y_single)\n",
        "sentiment_price_prediction = predict_next_5_days_test(model_sentiment, test_df_sentiment, scaler_sentiment, scaler_y_single)\n",
        "\n",
        "# print the results\n",
        "for i in range(1):\n",
        "  print(f\"Day {i+1}: Price Prediction = {price_prediction[i][0]} ----|---- Sentiment Prediction = {sentiment_price_prediction[i][0]}\")"
      ],
      "metadata": {
        "id": "HMagZ09sdkGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features for both models\n",
        "price_features = ['open', 'high', 'low', 'close', 'volume', 'day', 'month']\n",
        "sentiment_features = ['avg_sentiment', 'sentiment_ma3', 'sentiment_lag1', 'log_article_count', 'sentiment_volatility', 'has_sentiment']\n",
        "\n",
        "# Create datasets\n",
        "test_df_price = test_df[price_features].values\n",
        "test_df_sentiment = test_df[price_features + sentiment_features].values\n",
        "\n",
        "# scale the data\n",
        "test_df_price_scaled = scaler_price.transform(test_df_price)\n",
        "test_df_sentiment_scaled = scaler_sentiment.transform(test_df_sentiment)\n",
        "\n",
        "# create sequences for LSTM\n",
        "def create_sequences(data, lookback=21):\n",
        "    X_seq = []\n",
        "    for i in range(len(data) - lookback + 1):\n",
        "        X_seq.append(data[i:i + lookback])\n",
        "    return np.array(X_seq)\n",
        "\n",
        "# sequences for the price only model\n",
        "test_df_price_seq = create_sequences(test_df_price_scaled, lookback=21)\n",
        "\n",
        "# sequences for the sentiment model\n",
        "test_df_sentiment_seq = create_sequences(test_df_sentiment_scaled, lookback=21)\n",
        "\n",
        "# predict prices for next days\n",
        "price_prediction = predict_next_5_days_test(model_price, test_df_price, scaler_price, scaler_y_single)\n",
        "sentiment_price_prediction = predict_next_5_days_test(model_sentiment, test_df_sentiment, scaler_sentiment, scaler_y_single)\n",
        "\n",
        "price_prediction = scaler_y_single.inverse_transform(price_prediction)\n",
        "sentiment_price_prediction = scaler_y_single.inverse_transform(sentiment_price_prediction)\n",
        "\n",
        "# print the results\n",
        "for i in range(len(price_prediction)):\n",
        "    print(f\"Day {i+1}: Price Prediction = {price_prediction[i][0]:.2f} ----|---- Sentiment Prediction = {sentiment_price_prediction[i][0]:.2f}\")"
      ],
      "metadata": {
        "id": "xdPKFuIZd-R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_price_seq.shape"
      ],
      "metadata": {
        "id": "URdKBP74iZqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q9vN0-TTia6J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "finapp1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}